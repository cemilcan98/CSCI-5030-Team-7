{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","import datetime\n","import numpy as np\n","import pandas as pd\n","import random\n","from tqdm import tqdm\n","import tensorflow.keras\n","import tensorflow as tf\n","from sklearn.metrics import fbeta_score\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras.layers import Embedding,LSTM, TimeDistributed, Dense, Bidirectional\n","from tensorflow.keras.initializers import HeNormal, GlorotNormal, GlorotUniform\n","from nltk.translate.bleu_score import sentence_bleu\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv('preprocessed_15.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.options.display.max_colwidth = 500\n","data[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def preprocess(t, add_start_token, add_end_token):\n","\n","  if add_start_token == True and add_end_token == False:\n","    t = '<start>'+' '+t\n","  if add_start_token == False and add_end_token == True:\n","    t = t+' '+'<end>'\n","  if add_start_token == True and add_end_token == True:\n","    t = '<start>'+' '+t+' '+'<end>'\n","\n","  t = re.sub(' +', ' ', t)\n","  return t"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["encoder_input = [preprocess(line, add_start_token= True, add_end_token=True) for line in data['error']]\n","decoder_input = [preprocess(line, add_start_token= True, add_end_token=False) for line in data['correct']]\n","decoder_output = [preprocess(line, add_start_token= False, add_end_token=True) for line in data['correct']]\n","print(encoder_input[0])\n","print(decoder_input[0])\n","print(decoder_output[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#ENCODER INPUT\n","\n","tokenizer = Tokenizer(filters='', split=\" \")\n","tokenizer.fit_on_texts(encoder_input)\n","word_index = tokenizer.word_index #vocabulary\n","\n","max_length = max([ len(row.split(\" \")) for row in encoder_input ])\n","INPUT_ENCODER_LENGTH = max_length\n","\n","enc_input_encoded = tokenizer.texts_to_sequences(encoder_input)\n","enc_input_padded= pad_sequences(enc_input_encoded, maxlen=INPUT_ENCODER_LENGTH, padding=\"post\")\n","\n","print(enc_input_padded.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(encoder_input[0])\n","print(enc_input_padded[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#DECODER INPUT\n","decoder_data = decoder_input.copy()\n","decoder_data.extend(decoder_output)\n","\n","out_tokenizer = Tokenizer(filters='', split=\" \")\n","out_tokenizer.fit_on_texts(decoder_data)\n","word_index = out_tokenizer.word_index #vocabulary\n","\n","max_length = max([ len(row.split(\" \")) for row in decoder_input ])\n","INPUT_DECODER_LENGTH = max_length"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dec_input_encoded = out_tokenizer.texts_to_sequences(decoder_input)\n","dec_input_padded= pad_sequences(dec_input_encoded, maxlen=INPUT_DECODER_LENGTH, padding=\"post\", truncating = \"post\")\n","\n","print(dec_input_padded.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(decoder_input[0])\n","print(dec_input_padded[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dec_output_encoded = out_tokenizer.texts_to_sequences(decoder_output)\n","dec_output_padded= pad_sequences(dec_output_encoded, maxlen=INPUT_DECODER_LENGTH, padding=\"post\", truncating = \"post\")\n","\n","print(dec_output_padded.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(decoder_output[1])\n","print(dec_output_padded[1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Reference: https://fasttext.cc/docs/en/english-vectors.html\n","import io\n","\n","def load_vectors(fname):\n","    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    n, d = map(int, fin.readline().split())\n","    data = {}\n","    for line in fin:\n","        tokens = line.rstrip().split(' ')\n","        data[tokens[0]] = np.asarray(tokens[1:])#map(float, tokens[1:])\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embedding_index = load_vectors('wiki-news-300d-1M.vec')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#https://keras.io/examples/nlp/pretrained_word_embeddings/\n","word_index = tokenizer.word_index\n","num_tokens = len(word_index) + 2\n","embedding_dim = 300\n","hits = 0\n","misses = 0\n","\n","embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = embedding_index.get(word)\n","\n","    if type(embedding_vector) == np.ndarray and embedding_vector.shape[0] == 300:  \n","        embedding_matrix[i] = embedding_vector\n","        hits += 1\n","\n","    else:\n","        misses += 1\n","print(\"Converted %d words (%d misses)\" % (hits, misses))\n","np.save('GEC/in_embedding.npy', embedding_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_index = out_tokenizer.word_index\n","num_tokens = len(word_index) + 2\n","embedding_dim = 300\n","hits = 0\n","misses = 0\n","\n","embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = embedding_index.get(word)\n","\n","    if type(embedding_vector) == np.ndarray and embedding_vector.shape[0] == 300:  \n","        embedding_matrix[i] = embedding_vector\n","        hits += 1\n","\n","    else:\n","        misses += 1\n","print(\"Converted %d words (%d misses)\" % (hits, misses))\n","np.save('GEC/out_embedding.npy', embedding_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["in_embedding_matrix = np.load('GEC/in_embedding.npy')\n","out_embedding_matrix = np.load('GEC/out_embedding.npy')\n","print(in_embedding_matrix.shape, out_embedding_matrix.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#ENCODER\n","class Encoder(tf.keras.Model):\n","    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n","        super().__init__()\n","        self.vocab_size = inp_vocab_size\n","        self.embedding_size = embedding_size\n","        self.lstm_units = lstm_size\n","        self.input_length = input_length\n","\n","def build(self, input_sequence):\n","        #self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, input_length=self.input_length, \n","        #                           #embeddings_initializer=keras.initializers.Constant(in_embedding_matrix), mask_zero=True, \n","        #                           weights = [in_embedding_matrix], mask_zero=True, \n","        #                           trainable = False, name=\"embedding_layer_encoder\")\n","        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n","                           mask_zero=True, name=\"embedding_layer_encoder\")\n","        self.lstm = LSTM(self.lstm_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n","\n","    def call(self,input_sequence,states, training = True):\n","        input_embedding = self.embedding(input_sequence)   #(batch_size, length of input array, embedding_size)\n","        self.lstm_output, self.state_h, self.state_c = self.lstm(input_embedding, initial_state = states)\n","        return self.lstm_output,self.state_h, self.state_c\n","\n","\n","    def initialize_states(self,batch_size):\n","      initializer = GlorotNormal()\n","      lstm_state_h = initializer(shape=(batch_size, self.lstm_units))#tf.zeros((batch_size, self.lstm_units), dtype=tf.dtypes.float32, name=\"Encoder_LSTM_hidden_state\")\n","      lstm_state_c = initializer(shape=(batch_size, self.lstm_units))#tf.zeros((batch_size, self.lstm_units), dtype=tf.dtypes.float32, name=\"Encoder_LSTM_cell_state\")\n","      return lstm_state_h, lstm_state_c\n","\n","\n","#DECODER\n","class Decoder(tf.keras.Model):\n","    def init(self,out_vocab_size,embedding_size,lstm_size,input_length):\n","        super().init()\n","        self.vocab_size = out_vocab_size\n","        self.embedding_size = embedding_size\n","        self.lstm_units = lstm_size\n","        self.input_length = input_length\n","\n","\n","    def build(self,input_sequence):\n","        #self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, input_length=self.input_length, \n","        #                           #embeddings_initializer=keras.initializers.Constant(out_embedding_matrix), \n","        #                           weights = [out_embedding_matrix], mask_zero=True, \n","        #                           trainable = False, name=\"embedding_layer_decoder\")\n","        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n","                           mask_zero=True, name=\"embedding_layer_decoder\") \n","        self.lstm = LSTM(self.lstm_units, return_state=True, return_sequences=True, name=\"Decoder_LSTM\")\n","\n","\n","    def call(self,input_sequence,initial_states, training = True):\n","\n","        input_embedding = self.embedding(input_sequence)\n","        self.lstm_output, self.state_h, self.state_c = self.lstm(input_embedding, initial_state=initial_states)\n","        return self.lstm_output,self.state_h, self.state_c"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Encoder_decoder(tf.keras.Model):\n","    \n","    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size):\n","\n","        super().__init__()\n","        self.encoder = Encoder(INPUT_VOCAB_SIZE, embedding_size = 256, lstm_size= 1200 , input_length= INPUT_ENCODER_LENGTH)\n","        self.decoder = Decoder(OUTPUT_VOCAB_SIZE, embedding_size = 256, lstm_size = 1200, input_length = None)\n","        self.dense = Dense(output_vocab_size)#, activation = 'softmax')\n","    \n","    def call(self,data):\n","        input, output = data[0], data[1]\n","        states = self.encoder.initialize_states(input.shape[0])\n","        encoder_output,encoder_final_state_h,encoder_final_state_c = self.encoder(input, states)\n","        decoder_output,decoder_state_h,decoder_state_c = self.decoder(output,[encoder_final_state_h,encoder_final_state_c])\n","        outputs = self.dense(decoder_output)\n","\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["INPUT_VOCAB_SIZE = len(list(tokenizer.word_index)) +1 #for zero padding +OOV\n","OUTPUT_VOCAB_SIZE = len(list(out_tokenizer.word_index)) +1 #for zero padding + OOV\n","BATCH_SIZE = 16\n","print(INPUT_VOCAB_SIZE, INPUT_ENCODER_LENGTH, OUTPUT_VOCAB_SIZE, INPUT_DECODER_LENGTH, BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_dataset = full_dataset.take(50).batch(32)\n","train_dataset = full_dataset.skip(50).batch(32)\n","\n","print(train_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#LEARNING RATE SCHEDULER: Decay learning rate after 15 epochs\n","def scheduler(epoch, lr):\n","   if epoch < 1:\n","     return lr   \n","   else:\n","     return lr * tf.math.exp(-0.1)\n","lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n","\n","#EARLY STOPPING\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n","\n","#TENSORBOARD PLOTS\n","tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir='logs')\n","\n","#SAVE MODEL WEIGHTS\n","class SaveModel(tf.keras.callbacks.Callback):\n","\n","  def __init__(self):\n","    self.history = { 'loss' : [],  'val_loss' : []}\n","    self.init = 0\n","\n","  def on_epoch_end(self, epoch, logs = {}):\n","    \n","    self.history['loss'].append(logs.get('loss'))\n","    if logs.get('val_loss', -1) != -1:\n","        self.history['val_loss'].append(logs.get('val_loss'))\n","\n","    #if epochs % 10 == 0:\n","    self.model.save_weights('GEC/ENC_DEC_EMB/weights_{}.h5'.format(epoch+self.init))    #print('Saved weights for epoch {}!'.format(epoch))\n","\n","    df = pd.DataFrame(columns = ['loss','val_loss']) \n","    for col in df.columns:\n","      df[col] = self.history[col]\n","    df.to_csv('history.csv')\n","    !cp history.csv \"GEC//ENC_DEC_EMB/history.csv\"\n","\n","save_model = SaveModel()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#https://www.tensorflow.org/tutorials/text/image_captioning#model\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none'\n",")\n","\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def f_beta_score(y_true, y_pred):\n","  y_pred_sparse = tf.convert_to_tensor(np.argmax(y_pred, axis = -1), dtype = tf.float32)\n","  fb_score = [ fbeta_score(y_true[i], y_pred_sparse[i],average = 'macro',beta = 0.5) for i in range(y_true.shape[0])]#tf.py_function(fbeta_score, inp = [y, y_pred, 0.5], Tout=tf.float32)\n","  return sum(fb_score)/len(fb_score)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","tf.config.run_functions_eagerly(True)\n","\n","#Create an object of encoder_decoder Model class, \n","# Compile the model and fit the model\n","input = np.random.randint(0, 64, size=(BATCH_SIZE, INPUT_ENCODER_LENGTH))\n","output = np.random.randint(0, 64, size=(BATCH_SIZE, INPUT_DECODER_LENGTH))\n","target = np.random.randint(0, 64, size=(BATCH_SIZE, INPUT_DECODER_LENGTH))#tf.keras.utils.to_categorical(output, OUTPUT_VOCAB_SIZE)\n","\n","model = Encoder_decoder(encoder_inputs_length = INPUT_ENCODER_LENGTH, decoder_inputs_length =INPUT_DECODER_LENGTH, output_vocab_size= OUTPUT_VOCAB_SIZE)\n","#model = encoder_decoder(enc_units = 1024, dec_units = 1024, scoring_func = 'dot', att_units = 1024)\n","model.compile(optimizer=tf.keras.optimizers.Adam(),loss=loss_function, metrics = [f_beta_score])#tf.keras.metrics.categorical_crossentropy)\n","model.fit([input, output], target, steps_per_epoch=1)\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Input processor\n","def input_processor(input_sentence, pad_seq):\n","\n","  #Preprocess to remove unwanted characters and convert to ASCII characters\n","  encoder_input = preprocess(input_sentence, add_start_token= True, add_end_token=True)\n","\n","  #Convert to sequence\n","  tokenized_text = tokenizer.texts_to_sequences([encoder_input])\n","  if pad_seq == True:\n","    tokenized_text = pad_sequences(tokenized_text, maxlen=INPUT_ENCODER_LENGTH, padding=\"post\")\n","\n","  tokenized_text = tf.convert_to_tensor(tokenized_text, dtype = tf.float32)\n","  return tokenized_text\n","\n","\n","def remove_end_token(words):\n","  words_list = words.split(' ')[:-1]\n","  words = \" \".join(words_list)\n","  return words"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict(input_sentence):\n","  input = input_processor(input_sentence, pad_seq = False)\n","\n","  INPUT_LENGTH = input.shape[0] #Or number of inputs\n","\n","  states = model.layers[0].initialize_states(INPUT_LENGTH)\n","\n","  encoder_output,encoder_final_state_h,encoder_final_state_c = model.layers[0](input, states)\n","  states = [encoder_final_state_h,encoder_final_state_c]  #States to initialize Decoder with\n","\n","  input_decoder = np.zeros((1,1))\n","  input_decoder[0][0] = 2  #<start> for eng vocab\n","  \n","  decoder_output_list = []\n","  stop = False\n","\n","  while stop != True :\n","\n","    decoder_output, dec_final_state_h, dec_final_state_c = model.layers[1](input_decoder, states)\n","    \n","    states = [dec_final_state_h, dec_final_state_c]\n","\n","    output = model.layers[2](decoder_output)\n","\n","    index = np.argmax(output, -1)\n","    decoder_output_list.append(index)\n","    input_decoder = index\n","\n","    if index[0][0] == 4 :#or len(decoder_output_list) > input.shape[1]: #Index of <end> for out_tokenizer\n","      stop =True\n","\n","  #Get the output tokens and store in arr_out\n","  arr_out = [int(np.asarray(i)[0][0]) for i in decoder_output_list]\n","\n","  #Convert to text\n","  output_words = out_tokenizer.sequences_to_texts([arr_out])\n","\n","  return output_words\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n","# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n","\n","#Pick 1000 random indices\n","indices = [random.randint(0, 10000) for i in range(3)]\n","sent_list = [data['error'].iloc[i] for i in indices]\n","bleu_scores_ = []\n","actual_output = []\n","output_sent_list = []\n","\n","for i in range(len(sent_list)):\n","  print(sent_list[i])\n","\n","#Translate and calculate BLEU scores\n","for i, sent in enumerate(tqdm(sent_list)):\n","  out = predict(sent) \n","  print(\"\\nout is: \",out)\n","  actual_ = decoder_output[indices[i]]\n","\n","  output_sent_list.append(out[0])\n","  actual_output.append(actual_)\n","\n","  #Remove <end> token\n","  out_words = remove_end_token(out[0])\n","  actual_output_ = remove_end_token(actual_)\n","\n","  #Calculate BLEU scores\n","  bleu_scores_.append(sentence_bleu(actual_output_.split(' '), out_words.split(' ')))\n","\n","\n","print('\\nAverage BLEU score :',sum(bleu_scores_)/len(bleu_scores_))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(data['error']))\n","x = data['error']\n","print(x[10])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.DataFrame(columns= ['input_sentence', 'actual_output','translated_output', 'bleu_score'])\n","df['input_sentence'] = sent_list\n","df['actual_output'] = actual_output\n","df['translated_output'] = output_sent_list\n","df['bleu_score'] = bleu_scores_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[0:5]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.load_weights('GEC/ENC_DEC_EMB/weights_0.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n","# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n","\n","#Pick 1000 random indices\n","indices = [random.randint(0, 10000) for i in range(100)]\n","sent_list = [data['error'].iloc[i] for i in indices]\n","#sent_list = [\"Hello my name is amir\"]\n","bleu_scores_ = []\n","actual_output = []\n","output_sent_list = []\n","\n","print(sent_list)\n","\n","#Translate and calculate BLEU scores\n","for i, sent in enumerate(tqdm(sent_list)):\n","  out = predict(sent) \n","  actual_ = decoder_output[indices[i]]\n","\n","  output_sent_list.append(out[0])\n","  actual_output.append(actual_)\n","\n","  #Remove <end> token\n","  out_words = remove_end_token(out[0])\n","  actual_output_ = remove_end_token(actual_)\n","\n","  #Calculate BLEU scores\n","  bleu_scores_.append(sentence_bleu(actual_output_.split(' '), out_words.split(' ')))\n","\n","\n","print('Average BLEU score :',sum(bleu_scores_)/len(bleu_scores_))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.DataFrame(columns= ['input_sentence', 'actual_output','translated_output', 'bleu_score'])\n","df['input_sentence'] = sent_list\n","df['actual_output'] = actual_output\n","df['translated_output'] = output_sent_list\n","df['bleu_score'] = bleu_scores_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["user_file = open(\"MyFile.txt\", \"r\")\n","\n","list_of_lists = []\n","for line in user_file:\n","  list_of_lists.append(line)\n","\n","user_file.close()\n","\n","print(list_of_lists[0])\n","\n","out = predict(list_of_lists[0]) \n","\n","print(out)\n","x = str(out)\n","user_output = x[2:-8]\n","print(\"user_outputis \", user_output)\n","\n","f = open(\"demofile3.txt\", \"w\")\n","f.write(user_output)\n","f.close()\n","\n","#open and read the file after the appending:\n","f = open(\"demofile3.txt\", \"r\")\n","print(f.read())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras.models import load_model\n","\n","model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n","del model  # deletes the existing model\n","\n","# returns a compiled model\n","# identical to the previous one\n","model = load_model('my_model.h5')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"GEC_Baseline_Encoder_Decoder.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
